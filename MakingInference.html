<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Making Inference</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Math 325 Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Table of Contents</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Describing Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GraphicalSummaries.html">Graphical Summaries</a>
    </li>
    <li>
      <a href="NumericalSummaries.html">Numerical Summaries</a>
    </li>
    <li>
      <a href="GeneralRCommands.html">General R Commands</a>
    </li>
    <li>
      <a href="RMarkdownHints.html">General R Markdown Hints</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Making Inference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="MakingInference.html">Making Inference</a>
    </li>
    <li>
      <a href="tTests.html">t Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="ChiSquaredTests.html">Chi-Squared Tests</a>
    </li>
    <li>
      <a href="PermutationTests.html">Permutation Tests</a>
    </li>
  </ul>
</li>
<li>
  <a href="ClassNotes.html">Class Notes</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Critiquing Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Guide to Critiquing t Tests.html">T-Tests</a>
    </li>
    <li>
      <a href="Guide to Critiquing Wilcoxon Tests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="CritiquingANOVATests.html">ANOVA Tests</a>
    </li>
    <li>
      <a href="Guide to Critiquing Kruskal Tests.html">Guide to Critiquing Kruskal Tests</a>
    </li>
    <li>
      <a href="Critiquing Guide.html">Linear Regression</a>
    </li>
    <li>
      <a href="Critiquing Guide Logistic.html">Logistic Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Analyses/StudentHousingII.html">Student HousingII</a>
    </li>
    <li>
      <a href="./Analyses/Movies.html">Movies</a>
    </li>
    <li>
      <a href="./Analyses/IQTwins.html">IQ Twins</a>
    </li>
    <li>
      <a href="./Analyses/RecallingWords.html">Recalling Words- ANOVA</a>
    </li>
    <li>
      <a href="./Analyses/DayCare.html">DayCare- TWO-WAY ANOVA</a>
    </li>
    <li>
      <a href="./Analyses/ReadingComprehension.html">Reading Comp-Kruskal Wallis</a>
    </li>
    <li>
      <a href="./Analyses/MySimpleLinearRegression.html">SimpleLinearRegression</a>
    </li>
    <li>
      <a href="./Analyses/CarPrices.html">CarPrices-Multiple Linear Regression</a>
    </li>
    <li>
      <a href="./Analyses/Project1.html">Project1- Independent</a>
    </li>
    <li>
      <a href="./Analyses/MyLogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="./Analyses/Discrimination.html">Chi-Squared, Discrimination</a>
    </li>
    <li>
      <a href="./Analyses/Explosives.html">Permutations, Explosives</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Making Inference</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
</script>
<hr />
<p>It is common to only have a <strong>sample</strong> of data from some population of interest. Using the information from the sample to reach conclusions about the population is called <em>making inference</em>. When statistical inference is performed properly, the conclusions about the population are almost always correct.</p>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis Testing</h2>
<div style="padding-left:15px;">
<p>One of the great focal points of statistics concerns hypothesis testing. Science generally agrees upon the principle that truth must be uncovered by the process of elimination. The process begins by establishing a starting assumption, or <em>null hypothesis</em> (<span class="math inline">\(H_0\)</span>). Data is then collected and the evidence against the null hypothesis is measured, typically with the <span class="math inline">\(p\)</span>-value. The <span class="math inline">\(p\)</span>-value becomes small (gets close to zero) when the evidence is <em>extremely</em> different from what would be expected if the null hypothesis were true. When the <span class="math inline">\(p\)</span>-value is below the <em>significance level</em> <span class="math inline">\(\alpha\)</span> (typically <span class="math inline">\(\alpha=0.05\)</span>) the null hypothesis is abandoned (rejected) in favor of a competing <em>alternative hypothesis</em> (<span class="math inline">\(H_a\)</span>).</p>
<a href="javascript:showhide('progressionOfHypotheses')">Click for an Example </a>
<div id="progressionOfHypotheses" style="display:none;">
<p>The current hypothesis may be that the world is flat. Then someone who thinks otherwise sets sail in a boat, gathers some evidence, and when there is sufficient evidence in the data to disbelieve the current hypothesis, we conclude the world is not flat. In light of this new knowledge, we shift our belief to the next working hypothesis, that the world is round. After a while, someone gathers more evidence and shows that the world is not round, and we move to the next working hypothesis, that it is oblate spheroid, i.e., a sphere that is squashed at its poles and swollen at the equator.</p>
<div class="figure">
<img src="Images/progressionOfHypotheses.png" />

</div>
<p>This process of elimination is called hypothesis testing. The process begins by establishing a <em>null hypothesis</em> (denoted symbolically by <span class="math inline">\(H_0\)</span>) which represents the current opinion, status quo, or what we will believe if the evidence is not sufficient to suggest otherwise. The alternative hypothesis (denoted symbolically by <span class="math inline">\(H_a\)</span>) designates what we will believe if there is sufficient evidence in the data to discredit, or “reject,” the null hypothesis.</p>
<p>See the <a href="http://statistics.byuimath.com/index.php?title=Lesson_2:_The_Statistical_Process_%26_Design_of_Studies#Making_Inferences:_Hypothesis_Testing">BYU-I Math 221 Stats Wiki</a> for another example.</p>
</div>
<p><br /></p>
<h3 id="managing-decision-errors">Managing Decision Errors</h3>
<p>When the <span class="math inline">\(p\)</span>-value approaches zero, one of two things must be occurring. Either an extremely rare event has happened or the null hypothesis is incorrect. Since the second option, that the null hypothesis is incorrect, is the more plausible option, we reject the null hypothesis in favor of the alternative whenever the <span class="math inline">\(p\)</span>-value is close to zero. It is important to remember that rejecting the null hypothesis could however be a mistake.</p>
<div style="padding-left:30px; padding-right:10%;">
<table>
<thead>
<tr class="header">
<th> </th>
<th><span class="math inline">\(H_0\)</span> True</th>
<th><span class="math inline">\(H_0\)</span> False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reject</strong> <span class="math inline">\(H_0\)</span></td>
<td>Type I Error</td>
<td>Correct Decision</td>
</tr>
<tr class="even">
<td><strong>Accept</strong> <span class="math inline">\(H_0\)</span></td>
<td>Correct Decision</td>
<td>Type II Error</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
<h3 id="type-i-error-significance-level-confidence-and-alpha">Type I Error, Significance Level, Confidence and <span class="math inline">\(\alpha\)</span></h3>
<p>A <strong>Type I Error</strong> is defined as rejecting the null hypothesis when it is actually true. (Throwing away truth.) The <strong>significance level</strong>, <span class="math inline">\(\alpha\)</span>, of a hypothesis test controls the probability of a Type I Error. The typical value of <span class="math inline">\(\alpha = 0.05\)</span> came from tradition and is a somewhat arbitrary value. Any value from 0 to 1 could be used for <span class="math inline">\(\alpha\)</span>. When deciding on the level of <span class="math inline">\(\alpha\)</span> for a particular study it is important to remember that as <span class="math inline">\(\alpha\)</span> increases, the probability of a Type I Error increases, and the probability of a Type II Error decreases. When <span class="math inline">\(\alpha\)</span> gets smaller, the probability of a Type I Error gets smaller, while the probability of a Type II Error increases. <strong>Confidence</strong> is defined as <span class="math inline">\(1-\alpha\)</span> or the opposite of a Type I error. That is the probability of accepting the NULL when it is in fact true.</p>
<p><br /></p>
<h3 id="type-ii-errors-beta-and-power">Type II Errors, <span class="math inline">\(\beta\)</span>, and Power</h3>
<p>It is also possible to make a <strong>Type II Error</strong>, which is defined as failing to reject the null hypothesis when it is actually false. (Failing to move to truth.) The probability of a Type II Error, <span class="math inline">\(\beta\)</span>, is often unknown. However, practitioners often make an assumption about a detectable difference that is desired which then allows <span class="math inline">\(\beta\)</span> to be prescribed much like <span class="math inline">\(\alpha\)</span>. In essence, the detectable difference prescribes a fixed value for <span class="math inline">\(H_a\)</span>. We can then talk about the <strong>power</strong> of of a hypothesis test, which is 1 minus the probability of a Type II Error, <span class="math inline">\(\beta\)</span>. See <a href="https://en.wikipedia.org/wiki/Statistical_power">Statistical Power</a> in Wikipedia for a starting source if your are interested. <a href="http://rpsychologist.com/d3/NHST/" target="blank">This website</a> provides a novel interactive visualization to help you understand power. It does require a little background on <a href="http://rpsychologist.com/d3/cohend/">Cohen’s D</a>.</p>
<p><br /></p>
<h3 id="sufficient-evidence">Sufficient Evidence</h3>
<p>Statistics comes in to play with hypothesis testing by defining the phrase “sufficient evidence.” When there is “sufficient evidence” in the data, the null hypothesis is rejected and the alternative hypothesis becomes the working hypothesis.</p>
<p>There are many statistical approaches to this problem of measuring the significance of evidence, but in almost all cases, the final measurement of evidence is given by the <span class="math inline">\(p\)</span>-value of the hypothesis test. The <span class="math inline">\(p\)</span>-value of a test is defined as the probability of the evidence being as extreme or more extreme than what was observed assuming the null hypothesis is true. This is an interesting phrase that is at first difficult to understand.</p>
<p>The “as extreme or more extreme” part of the definition of the <span class="math inline">\(p\)</span>-value comes from the idea that the null hypothesis will be rejected when the evidence in the data is extremely inconsistent with the null hypothesis. If the data is not extremely different from what we would expect under the null hypothesis, then we will continue to believe the null hypothesis. Although, it is worth emphasizing that this does not prove the null hypothesis to be true.</p>
<p><br /></p>
<h3 id="evidence-not-proof">Evidence not Proof</h3>
<p>Hypothesis testing allows us a formal way to decide if we should “conclude the alternative” or “<em>continue</em> to accept the null.” It is important to remember that statistics (and science) cannot <em>prove</em> anything, just show evidence towards. Thus we never really <em>prove</em> a hypothesis is true, we simply show evidence towards or against a hypothesis.</p>
</div>
<p><br /></p>
</div>
<div id="pvalue" class="section level2">
<h2>Calculating the <span class="math inline">\(p\)</span>-Value</h2>
<div style="padding-left:15px;">
<p>Recall that the <span class="math inline">\(p\)</span>-value measures how extremely the data (the evidence) differs from what is expected under the null hypothesis. Small <span class="math inline">\(p\)</span>-values lead us to discard (reject) the null hypothesis.</p>
<p>A <span class="math inline">\(p\)</span>-value can be calculated whenever we have two things.</p>
<ol style="list-style-type: decimal">
<li><p>A <em>test statistic</em>, which is a way of measuring how “far” the observed data is from what is expected under the null hypothesis.</p></li>
<li><p>The <em>sampling distribution</em> of the test statistic, which is the theoretical distribution of the test statistic over all possible samples, assuming the null hypothesis was true.</p></li>
</ol>
<p>A <em>distribution</em> describes how data is spread out. When we know the shape of a distribution, we know which values are possible, but more importantly which values are most plausible (likely) and which are the least plausible (unlikely). The <span class="math inline">\(p\)</span>-value uses the <em>sampling distribution</em> of the test statistic to measure the probability of the observed test statistic being as extreme or more extreme than the one observed.</p>
<p>All <span class="math inline">\(p\)</span>-value computation methods can be classified into two broad categories, <em>parametric</em> methods and <em>nonparametric</em> methods.</p>
<h3 id="parametric-methods">Parametric Methods</h3>
<div style="padding-left:15px;">
<p>Parametric methods assume that, under the null hypothesis, the test statistic follows a specific theoretical parametric distribution. Parametric methods are typically more statistically powerful than nonparametric methods, but necessarily force more assumptions on the data.</p>
<p><em>Parametric distributions</em> are theoretical distributions that can be described by a mathematical function. There are many theoretical distributions. (See the <a href="https://en.wikipedia.org/wiki/List_of_probability_distributions">List of Probability Distributions</a> in Wikipedia for details.) Some of the most widely used distributions are described below.</p>
<hr />
<h4 id="normal">The Normal Distribution</h4>
<p>One of the most important distributions in statistics is the normal distribution. It is a theoretical distribution that approximates the distributions of many real life data distributions, like heights of people, heights of corn plants, baseball batting averages, lengths of gestational periods for many species including humans, and so on.</p>
<p>More importantly, the <em>sampling distribution</em> of the sample mean <span class="math inline">\(\bar{x}\)</span> is normally distributed in two important scenarios.</p>
<ol style="list-style-type: decimal">
<li>The parent population is normally distributed.</li>
<li>The sample size is sufficiently large. (Often <span class="math inline">\(n\geq 30\)</span> is sufficient, but this is a general rule of thumb that is sometimes insufficient.)</li>
</ol>
<a href="javascript:showhide('normaldata')">Click here for more information about Normal Data </a>
<div id="normaldata" style="display:none;">
<p>It is often the case that we can assume that data comes from a normal distribution. Heights of people, plants, or animals are typically normally distributed. The distribution of the lengths of pregnancies (gestational periods) are normally distributed for most species. For humans the average pregnancy is roughly “9 months,” or more specifically, 266 days. Measurement errors are often normally distributed. For example if a person was to be weighed on a scale 30 different times, the measurements of their weight would probably not always be the same, especially if the scale was accurate enough. Despite being different, most measurements would likely not differ too dramatically from each other. Some would be a little higher and some a little lower, but the majority of the data would likely be close to the center. This is what it means when data is normally distributed, that most of it is close to the mean, some a little higher and some a little lower.</p>
<p><br /></p>
<h4 id="an-example">An Example</h4>
<p>To view a specific case, consider the following histogram of Major League Baseball batting averages (from 2009) which can be assumed to be normally distributed with a mean of <span class="math inline">\(\mu = 0.2612\)</span> and a standard deviation of <span class="math inline">\(\sigma=0.0339\)</span>. There are two important elements to the histogram, which is overlaid with a normal density curve, that is shown below. First, the <em>distribution</em> of the actual data is shown by the histogram. This is what actually occurred. It is real and unarguable. Second, the normal density curve that is overlaid on this histogram is a mathematical (theoretical) function that does a pretty good job of summarizing, or generalizing, the distribution of the data. It is theoretical and therefore arguable. No one actually knows if batting averages are normally distributed or not. However, the approximation of the normal curve to the histogram looks very reasonable, so we go ahead and “claim” that batting averages are normally distributed.</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<h2 id="normalprop">Properties of the Normal Distribution</h2>
<p>The reason we often “claim” that data is normally distributed is to apply the powerful properties of the mathematical function known as the normal density curve, or normal distribution. The mathematical formula that describes this function was uncovered by many early mathematicians, but Carl Friedrich Gauss published it in 1809 and therefore is usually given credit for its discovery. The function, which is perhaps a little intimidating at first, is given by <span class="math display">\[
  f(x |\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\]</span> Familiarity with all the symbols should make this function less intimidating. The two <em>parameters</em> of this mathematical function are denoted by the Greek letters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. The symbol <span class="math inline">\(\pi\)</span> is the Greek letter “pi” and is the famous constant, 3.14159… <span class="math inline">\(e\)</span> is also a famous constant occurring in nature and is given by 2.71828…. How amazing is that to have <span class="math inline">\(\pi\)</span> and <span class="math inline">\(e\)</span> in the same formula! Finally, <span class="math inline">\(x\)</span> is the value of <span class="math inline">\(x\)</span> along the x-axis and goes from negative infinity <span class="math inline">\((-\infty)\)</span> to positive infinity <span class="math inline">\((\infty)\)</span>. The answer of the right side after everything is filled in and calculated out is the height of the normal curve. The higher the normal curve above a certain value of <span class="math inline">\(x\)</span>, the more likely that value of <span class="math inline">\(x\)</span> is to happen.</p>
<p>Notice how the parameter <span class="math inline">\(\mu\)</span> controls the center of this distribution while the parameter <span class="math inline">\(\sigma\)</span> controls how spread out the distribution is. When <span class="math inline">\(\sigma\)</span> is larger, the resulting normal curve is flatter and more spread out (i.e., the data is more variable). When <span class="math inline">\(\sigma\)</span> is smaller, the resulting normal curve is taller and less spread out (i.e., the data is less variable). In any case, the most likely values of <span class="math inline">\(x\)</span> to occur are those that are close to <span class="math inline">\(\mu\)</span>. This is seen by noting that for any normal curve, it is tallest around its mean.</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The above plot and discussion demonstrates that just two parameters completely describe the normal distribution, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. What is even more powerful about the normal distribution is that any normal distribution can be re-scaled to the <em>standard normal distribution</em> by using the following formula <span class="math display">\[
  Z = \frac{X-\mu}{\sigma}
\]</span> Thus, whatever the mean <span class="math inline">\((\mu)\)</span> and standard deviation <span class="math inline">\((\sigma)\)</span> of a certain normal distribution, we can transform it to the <em>standard normal distribution</em> which looks like this</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The reason we would do this is because the standard normal distribution has the following property, roughly 68% of the data is within 1 standard deviation of the mean; roughly 95% of the data occurs within 2 standard deviations of the mean; and roughly 99.7% of the data occurs within 3 standard deviations of the mean. Thus, by 3 standard deviations, 997 of every 1,000 data points will have occurred. That means that fewer than 3 observations for every 1,000 will be more than three standard deviations from the mean if the data is truly normally distributed. This is shown graphically in the following plot.</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Further, the probability of any region of the normal density function can be obtained by calculating the area under the curve for a specific range of <span class="math inline">\(x\)</span>-values of interest.</p>
<h4 id="an-example-1">An Example</h4>
<p>Continuing with the previous example of batting averages, the probability that a randomly selected MLB player from 2009 has a batting average higher than 0.300 would be given by the shaded region in the plot below, which is 0.1262 (or 12.62%). In comparison, if we were to calculate the probability using our actual sample data, we would find that 0.1345 (or 13.45%) of the sample data was greater than or equal to 0.300. Thus the normal curve is a useful approximation to the actual sample data, but more importantly allows a generalization of the data to the full population. The question of interest thus becomes, how can the normal distribution be used to effectively make conclusions about the population based on the information provided by the sample?</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<h2 id="samplemeandist">The Sampling Distribution of the Sample Mean</h2>
<p>One of the most powerful results in statistics is the conclusion that the theoretical distribution of all possible sample means, the <em>sampling distribution of the sample mean</em>, is <strong>normal</strong> under two important scenarios!</p>
<ol style="list-style-type: decimal">
<li><p>If the parent population data is normal, then the sampling distribution of the sample mean will be normal.</p></li>
<li><p>If the sample size is large enough, then the sampling distribution of the sample mean will be (at least approximately) normal.</p></li>
</ol>
<p>If either of these above conditions is satisfied, then it follows that the distribution of the sample mean <span class="math inline">\((\bar{X})\)</span> will be (at least approximately) normally distributed. Let’s explore what this means more specifically.</p>
<p>Here at BYU-Idaho there are roughly 7,500 male students (<a href="http://www2.byui.edu/IR/stats/index.htm">click here</a> to see the actual data). If a random sample of size <span class="math inline">\(n=50\)</span> male students is taken from the population of 7,500 individuals, how many different samples are possible? It turns out that the number that describes how many possible samples of size <span class="math inline">\(n=50\)</span> can be taken from a population of 7,500 students has 130 digits in it! (The actual number is roughly 1.580873e+129.) This might be described as “a lot” of possible samples. For reference, note that the number “one million” has just 7 digits in it: 1,000,000 or 1.0e+6. Certainly it is <em>possible</em> that the sample gives us the 50 tallest males at BYU-Idaho. It is also <em>possible</em> that the sample consists of the shortest 50 males at BYU-Idaho. However, the fantastic result is that for the most part, most sample means will be “close” to the true population mean, with some being a little higher, and some being a little lower. In other words, the distribution of all theoretically possible sample means is <em>normal</em>.</p>
<h4 id="an-example-2">An Example</h4>
<p>To see this more clearly, let’s return to our batting averages example. As we saw previously, batting averages can be assumed to be normally distributed with a mean of 0.2612 and a standard deviation of 0.0339. To reach this conclusion we essentially looked at the data for the entire population. It would be nice if we could have reached this conclusion (or a nearly identical conclusion) using just a sample of say <span class="math inline">\(n=20\)</span> batting averages. For the batting averages data, the population consists of 446 individual batting averages. It turns out that there are 2.587035e+34 possible samples of size 20 that could be obtained from a population of 446 individuals. That is a lot of possible samples. Let’s compute the sample mean for a few of these, say for 1,000 of the possible samples. The distribution of the 1,000 resulting sample means are plotted over top of the histogram for the actual batting average data. Notice that this new histogram is centered around the original value of <span class="math inline">\(\mu=0.2612\)</span>. In fact, the mean of the 1,000 sample means comes out to be <span class="math inline">\(\mu_{\bar{x}} = 0.26135\)</span>, which is very close to the actual mean <span class="math inline">\(\mu=0.2612\)</span>. If we do the same thing for 10,000 random samples, then <span class="math inline">\(\mu_{\bar{x}} = 0.26126\)</span> and if we do it again for 20,000 samples we get <span class="math inline">\(\mu_{\bar{x}}=0.26123\)</span>. Notice that the larger the sample size gets, the closer that the mean of the sample means gets to the actual population mean. (Try saying that phrase out loud.) Thus we could say that “the mean of the means is the mean.”</p>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The conclusion of the above discussion is that if we took a single random sample of <span class="math inline">\(n=20\)</span> batting averages from the population of all batting averages, we would have ended up with an <span class="math inline">\(\bar{x}\)</span> ranging anywhere from somewhere around 0.2406 to 0.2840. However, more likely than not, the value would have been something closer to the true value of <span class="math inline">\(\mu=0.2612\)</span> than either of those numbers because the sample means are normally distributed around the population (true) mean.</p>
</div>
<h5 id="mathematical-formula">Mathematical Formula</h5>
<p><span class="math display">\[
  f(x | \mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\]</span> The symbols <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the two <em>parameters</em> of this distribution. The parameter <span class="math inline">\(\mu\)</span> controls the center, or mean of the distribution. The parameter <span class="math inline">\(\sigma\)</span> controls the spread, or standard deviation of the distribution.</p>
<h5 id="graphical-form">Graphical Form</h5>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<h5 id="comments">Comments</h5>
<p>The usefulness of the normal distribution is that we know which values of data are likely and which are unlikely by just knowing three things:</p>
<ol style="list-style-type: decimal">
<li><p>that the data is normally distributed,</p></li>
<li><p><span class="math inline">\(\mu\)</span>, the mean of the distribution, and</p></li>
<li><p><span class="math inline">\(\sigma\)</span>, the standard deviation of the distribution.</p></li>
</ol>
<p>For example, as shown in the plot above, a value of <span class="math inline">\(x=-8\)</span> would be very probable for the normal distribution with <span class="math inline">\(\mu=-5\)</span> and <span class="math inline">\(\sigma=2\)</span> (light blue curve). However, the value of <span class="math inline">\(x=-8\)</span> would be very unlikely to occur in the normal distribution with <span class="math inline">\(\mu=3\)</span> and <span class="math inline">\(\sigma=3\)</span> (gray curve). In fact, <span class="math inline">\(x=-8\)</span> would be even more unlikely an occurance for the <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span> distribution (dark blue curve).</p>
<p><br /> <br /></p>
<hr />
<h4 id="chisquared">The Chi Squared Distribution</h4>
<p>The <em>chi squared</em> distribution only allows for values that are greater than or equal to zero. While it has a few real life applications, by far its greatest use is theoretical.</p>
<p>The test statistic of the chi squared test is distributed according to a chi squared distribution.</p>
<h5 id="mathematical-formula-1">Mathematical Formula</h5>
<p><span class="math display">\[
  f(x|p) = \frac{1}{\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2}
\]</span> The only parameter of the chi squared distribution is <span class="math inline">\(p\)</span>, which is known as the degrees of freedom. Larger values of the parameter <span class="math inline">\(p\)</span> move the center of the chi squared distribution farther to the right. As <span class="math inline">\(p\)</span> goes to infinity, the chi squared distribution begins to look more and more normal in shape.</p>
<p>Note that the symbol in the denominator of the chi squared distribution, <span class="math inline">\(\Gamma(p/2)\)</span>, is the Gamma function of <span class="math inline">\(p/2\)</span>. (See <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma Function</a> in Wikipedia for details.)</p>
<h5 id="graphical-form-1">Graphical Form</h5>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<h5 id="comments-1">Comments</h5>
<p>It is important to remember that the chi squared distribution is only defined for <span class="math inline">\(x\geq 0\)</span> and for positive values of the parameter <span class="math inline">\(p\)</span>. This is unlike the normal distribution which is defined for all numbers <span class="math inline">\(x\)</span> from negative infinity to positive infinity as well as for all values of <span class="math inline">\(\mu\)</span> from negative infinity to positive infinity.</p>
<p><br /> <br /></p>
<hr />
<h4 id="the-t-distribution">The t Distribution</h4>
<p>A close friend of the normal distribution is the t distribution. Although the t distribution is seldom used to model real life data, the distribution is used extensively in hypothesis testing. For example, it is the sampling distribution of the one sample t statistic. It also shows up in many other places, like in regression, in the independent samples t test, and in the paired samples t test.</p>
<h5 id="mathematical-formula-2">Mathematical Formula</h5>
<p><span class="math display">\[
  f(x|p) = \frac{\Gamma\left(\frac{p+1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)}\frac{1}{\sqrt{p\pi}}\frac{1}{\left(1 + \left(\frac{x^2}{p}\right)\right)^{(p+1)/2}}
\]</span></p>
<p>Notice that, similar to the chi squared distribution, the t distribution has only one parameter, the degrees of freedom <span class="math inline">\(p\)</span>. As the single parameter <span class="math inline">\(p\)</span> is varied from <span class="math inline">\(p=1\)</span>, to <span class="math inline">\(p=2\)</span>, …, <span class="math inline">\(p=5\)</span>, and larger and larger numbers, the resulting distribution becomes more and more normal in shape.</p>
<p>Note that the expressions <span class="math inline">\(\Gamma\left(\frac{p+1}{2}\right)\)</span> and <span class="math inline">\(\Gamma(p/2)\)</span>, refer to the Gamma function. (See <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma Function</a> in Wikipedia for details.)</p>
<h5 id="graphical-form-2">Graphical Form</h5>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<h5 id="comments-2">Comments</h5>
<p>When the degrees of freedom <span class="math inline">\(p=30\)</span>, the resulting t distribution is almost indistinguishable visually from the normal distribution. This is one of the reasons that a sample size of 30 is often used as a rule of thumb for the sample size being “large enough” to assume the sampling distribution of the sample mean is approximately normal.</p>
<p><br /> <br /></p>
<hr />
<h4 id="fdist">The F Distribution</h4>
<p>Another commonly used distribution for test statistics, like in ANOVA and regression, is the F distribution. Technically speaking, the F distribution is the ratio of two chi squared random variables that are each divided by their respective degrees of freedom.</p>
<h5 id="mathematical-formula-3">Mathematical Formula</h5>
<p><span class="math display">\[
  f(x|p_1,p_2) = \frac{\Gamma\left(\frac{p_1+p_2}{2}\right)}{\Gamma\left(\frac{p_1}{2}\right)\Gamma\left(\frac{p_2}{2}\right)}\frac{\left(\frac{p_1}{p_2}\right)^{p_1/2}x^{(p_1-2)/2}}{\left(1+\left(\frac{p_1}{p_2}\right)x\right)^{(p_1+p_2)/2}}
\]</span> where <span class="math inline">\(x\geq 0\)</span> and the parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are the “numerator” and “denominator” degrees of freedom, respectively.</p>
<h5 id="graphical-form-3">Graphical Form</h5>
<p><img src="MakingInference_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<h5 id="comments-3">Comments</h5>
<p>The effects of the parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> on the F distribution are complicated, but generally speaking, as they both increase the distribution becomes more and more normal in shape.</p>
<p><br /></p>
</div>
<hr />
<h3 id="nonparametric-methods">Nonparametric Methods</h3>
<div style="padding-left:15px;">
<p>Nonparametric methods place minimal assumptions on the distribution of data. They allow the data to “speak for itself.” They are typically less powerful than the parametric alternatives, but are more broadly applicable because fewer assumptions need to be satisfied. Nonparametric methods include <a href="WilcoxonTests.html">Rank Sum Tests</a> and <a href="PermutationTests.html">Permutation Tests</a>.</p>
</div>
</div>
<footer>
</footer>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
