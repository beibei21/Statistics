<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Linear Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Math 325 Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Table of Contents</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Describing Data
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GraphicalSummaries.html">Graphical Summaries</a>
    </li>
    <li>
      <a href="NumericalSummaries.html">Numerical Summaries</a>
    </li>
    <li>
      <a href="GeneralRCommands.html">General R Commands</a>
    </li>
    <li>
      <a href="RMarkdownHints.html">General R Markdown Hints</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Making Inference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="MakingInference.html">Making Inference</a>
    </li>
    <li>
      <a href="tTests.html">t Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="ChiSquaredTests.html">Chi-Squared Tests</a>
    </li>
    <li>
      <a href="PermutationTests.html">Permutation Tests</a>
    </li>
  </ul>
</li>
<li>
  <a href="ClassNotes.html">Class Notes</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Critiquing Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Guide to Critiquing t Tests.html">T-Tests</a>
    </li>
    <li>
      <a href="Guide to Critiquing Wilcoxon Tests.html">Wilcoxon Tests</a>
    </li>
    <li>
      <a href="CritiquingANOVATests.html">ANOVA Tests</a>
    </li>
    <li>
      <a href="Guide to Critiquing Kruskal Tests.html">Guide to Critiquing Kruskal Tests</a>
    </li>
    <li>
      <a href="Critiquing Guide.html">Linear Regression</a>
    </li>
    <li>
      <a href="Critiquing Guide Logistic.html">Logistic Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="./Analyses/StudentHousingII.html">Student HousingII</a>
    </li>
    <li>
      <a href="./Analyses/Movies.html">Movies</a>
    </li>
    <li>
      <a href="./Analyses/IQTwins.html">IQ Twins</a>
    </li>
    <li>
      <a href="./Analyses/RecallingWords.html">Recalling Words- ANOVA</a>
    </li>
    <li>
      <a href="./Analyses/DayCare.html">DayCare- TWO-WAY ANOVA</a>
    </li>
    <li>
      <a href="./Analyses/ReadingComprehension.html">Reading Comp-Kruskal Wallis</a>
    </li>
    <li>
      <a href="./Analyses/MySimpleLinearRegression.html">SimpleLinearRegression</a>
    </li>
    <li>
      <a href="./Analyses/CarPrices.html">CarPrices-Multiple Linear Regression</a>
    </li>
    <li>
      <a href="./Analyses/Project1.html">Project1- Independent</a>
    </li>
    <li>
      <a href="./Analyses/MyLogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="./Analyses/Discrimination.html">Chi-Squared, Discrimination</a>
    </li>
    <li>
      <a href="./Analyses/Explosives.html">Permutations, Explosives</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Linear Regression</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
</script>
<hr />
<p>Determine which explanatory variables have a significant effect on the mean of the quantitative response variable.</p>
<hr />
<div id="simple-linear-regression" class="section level3 tabset tabset-fade tabset-pills">
<h3>Simple Linear Regression</h3>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYQuantX.png" width=58px;></p>
</div>
<p>Simple linear regression is a good analysis technique when the data consists of a single quantitative response variable <span class="math inline">\(Y\)</span> and a single quantitative explanatory variable <span class="math inline">\(X\)</span>.</p>
<div id="overview" class="section level4">
<h4>Overview</h4>
<div style="padding-left:125px;">
<p>The mathematical model of simple linear regression is <span class="math display">\[
  Y_i = \underbrace{\overbrace{\beta_0}^{\text{intercept}} + \overbrace{\beta_1}^{\text{slope}} X_i \ }_{\text{regression relation}} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_{i} \sim N(0,\sigma^2)\)</span> is the error term.</p>
<p>This model is appropriate when five assumptions can be made.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear Relation</strong>: the regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</p></li>
<li><p><strong>Normal Errors</strong>: the error terms are normally distributed with a mean of zero.</p></li>
<li><p><strong>Constant Variance</strong>: the variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</p></li>
<li><p><strong>Fixed X</strong>: the <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</p></li>
<li><p><strong>Independent Errors</strong>: the error terms are independent.</p></li>
</ol>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab for details about checking the regression assumptions.</p>
</div>
<p><strong>Hypotheses</strong></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}^{\quad \text{(most common)}}\quad\quad
\]</span></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}^{\quad\text{(sometimes useful)}}
\]</span></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>, claiming <span class="math inline">\(X\)</span> does not improve our understanding of the mean of <span class="math inline">\(Y\)</span> if the null hypothesis is true.</p>
<p>If <span class="math inline">\(\beta_0 = 0\)</span>, then the model reduces to <span class="math inline">\(Y_i = \beta_1 X + \epsilon_i\)</span>, claiming the average <span class="math inline">\(Y\)</span>-value is <span class="math inline">\(0\)</span> when <span class="math inline">\(X=0\)</span>.</p>
<hr />
</div>
</div>
<div id="r-instructions" class="section level4">
<h4>R Instructions</h4>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><code>mylm &lt;- lm(y ~ x, data=YourDataSet)</code> <strong>Perform the Regression</strong></p>
<p><code>summary(mylm)</code> <strong>View the Hypothesis Test Results</strong></p>
<p><code>plot(mylm, which=1:2)</code> <strong>Check Assumptions 1, 2, and 3</strong></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results of the <code>lm()</code> test. Note that <code>lm()</code> stands for “linear model.”</li>
<li><code>y</code> must be a “numeric” vector of the quantitative response variable.</li>
<li><code>x</code> is the explanatory variable. It can either be quantitative (most usual) or qualitative.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
</ul>
<p>To add the regression line to a scatterplot, first make the scatterplot, then use the command</p>
<p><code>abline(mylm)</code></p>
<p>Finally, note that the <code>mylm</code> object contains the <code>names(mylm)</code> of</p>
<ul>
<li><code>mylm$coefficients</code> Contains two values. The first is the estimated <span class="math inline">\(y\)</span>-intercept. The second is the estimated slope.</li>
<li><code>mylm$residuals</code> Contains the residuals from the regression in the same order as the actual dataset.</li>
<li><code>mylm$fitted.values</code> The values of <span class="math inline">\(\hat{Y}\)</span> in the same order as the original dataset.</li>
<li><code>mylm$...</code> several other things that will not be explained here.</li>
</ul>
<hr />
</div>
</div>
<div id="explanation" class="section level4">
<h4>Explanation</h4>
<div style="padding-left:125px;">
<h5 id="the-true-unknown-model-for-y_i">The True (unknown) Model for <span class="math inline">\(Y_i\)</span></h5>
<p>The mathematical model of simple linear regression is <span class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{\text{regression relation}} + \underbrace{\epsilon_i}_{\substack{\text{statistical} \\ \text{relation}}}
\]</span> where <span class="math inline">\(\epsilon_{i} \sim N(0,\sigma^2)\)</span> is the error term.</p>
<p>The model has two elements to it, a regression relation <span class="math inline">\(E\{Y\} = \beta_0 + \beta_1 X\)</span> and a statistical (i.e., a random) relation <span class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li><p>The regression relation <span class="math inline">\(E\{Y\} = \beta_0 + \beta_1 X\)</span> creates the line of regression where <span class="math inline">\(\beta_0\)</span> is the <span class="math inline">\(y\)</span>-intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope of the line. The regression relationship provides the average <span class="math inline">\(Y\)</span>-value for a given <span class="math inline">\(X\)</span>-value.</p></li>
<li><p>The statistical relation, the <span class="math inline">\(\epsilon_i\)</span>, allows the individual points to deviate from the line.</p></li>
</ul>
<p>Typically, the term “expected value” is used instead of the term “average” as another way of saying the mean, or average value. The notation for the expected value is <span class="math inline">\(E\{Y\}\)</span>. Thus, <span class="math display">\[
  E\{Y\} = \underbrace{\beta_0 + \beta_1 X}_{\text{regression relation}}
\]</span></p>
<h5 id="the-estimated-model-haty_i">The Estimated Model <span class="math inline">\(\hat{Y}_i\)</span></h5>
<p>An estimate of the unkown regression relation can be obtained from a sample of data. The estimate of the regression relation is given by <span class="math inline">\(\hat{Y}_i\)</span>, pronounced “<span class="math inline">\(Y\)</span> hat sub <span class="math inline">\(i\)</span>,” which is defined by the equation <span class="math display">\[
  \hat{Y}_i = b_0 + b_1 X_i 
\]</span> Here <span class="math inline">\(b_0\)</span> is the estimate of the true <span class="math inline">\(y\)</span>-intercept <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(b_1\)</span> is the estimate of the true slope <span class="math inline">\(\beta_1\)</span>. Note that the <span class="math inline">\(b\)</span>’s are sample statistics like <span class="math inline">\(\bar{x}\)</span> and the <span class="math inline">\(\beta\)</span>’s are population parameters like <span class="math inline">\(\mu\)</span>. The <span class="math inline">\(b\)</span>’s estimate the <span class="math inline">\(\beta\)</span>’s.</p>
<p>The formula for the estimate of the regression line (the regression relation) is <span class="math display">\[
  \hat{Y}_i = \underbrace{b_0 + b_1 X_i}_{\text{estimated relation}}
\]</span> Thus, <span class="math inline">\(\hat{Y}\)</span> is the estimator of <span class="math inline">\(E\{Y\}\)</span>. So <span class="math inline">\(\hat{Y}\)</span> is interpreted as the estimated average <span class="math inline">\(Y\)</span>-value for any given <span class="math inline">\(X\)</span>-value.</p>
<h5 id="putting-it-all-together">Putting it all Together</h5>
<p>This graphic depicts the true, but typically unknown, regression relation (dotted line). It also shows how a sample of data from the true regression relation (points) can be used to obtain an estimated regression equation (solid line) that is fairly close to the truth (dotted line).</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<h5 id="assumptions">Assumptions</h5>
<p>There are five assumptions that should be met for the mathematical model of simple linear regression to be appropriate.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
</ol>
<p><br /></p>
<h5 id="check">Checking the Assumptions</h5>
<p>Residuals are used to diagnose departures from the regression assumptions. Residuals are the difference between the observed value of <span class="math inline">\(Y\)</span> (the points) and the predicted, or estimated value, <span class="math inline">\(\hat{Y}\)</span>. Denoting a residual by <span class="math inline">\(r_i\)</span>, <span class="math display">\[
  r_i = Y_i - \hat{Y}_i
\]</span> The residual <span class="math inline">\(r_i\)</span> estimates the true error <span class="math inline">\(\epsilon_i\)</span>.</p>
<div style="padding-left:15px;">
<p><br /></p>
<h6 id="residuals-versus-fitted-values-plot-checks-assumptions-1-and-3">Residuals versus Fitted-values Plot: Checks Assumptions 1 and 3</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-2-1.png" width="144" />
</td>
<td width="75%">
<p>The linear relationship and constant variances assumptions can be diagnosed using a residuals versus fitted-values plot. The fitted values are the <span class="math inline">\(\hat{Y}_i\)</span>. The residuals versus fitted-values plot compares the residual to the magnitude of the fitted-value.</p>
<p>| <a href="./Analyses/ResidualsFittedPlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
<p><br /></p>
<h6 id="q-q-plot-of-the-residuals-checks-assumption-2">Q-Q Plot of the Residuals: Checks Assumption 2</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="144" />
</td>
<td width="75%">
<p>The normality of the error terms can be assessed by considering a normal probability plot (Q-Q Plot) of the residuals. If the residuals appear to be normal, then the error terms are also considered to be normal. If the residuals do not appear to be normal, then the error terms are also assumed to violate the normality assumption.</p>
<p>| <a href="./Analyses/NormalProbabilityPlots.html">Explanation</a> |</p>
</td>
</tr>
</table>
<p><br /></p>
<h6 id="residuals-versus-order-plot-checks-assumption-5">Residuals versus Order Plot: Checks Assumption 5</h6>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="144" />
</td>
<td width="75%">
<p>When the data is collected in a specific order, or has some other important ordering to it, then the independence of the error terms can be assessed. This is typically done by plotting the residuals against their order of occurrance. If any dramatic trends are visible in the plot, then the independence assumption is violated.</p>
<p>| <a href="./Analyses/ResidualsOrderPlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
</div>
<p><br /></p>
<h5 id="interpreting-the-model-parameters">Interpreting the Model Parameters</h5>
<p>The interpretation of <span class="math inline">\(\beta_0\)</span> is only meaningful if <span class="math inline">\(X=0\)</span> is in the scope of the model. If <span class="math inline">\(X=0\)</span> is in the scope of the model, then the intercept is interpreted as the <span class="math inline">\(E\{Y\}\)</span> when <span class="math inline">\(X=0\)</span>. The interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the <span class="math inline">\(E\{Y\}\)</span> per unit change in <span class="math inline">\(X\)</span>.</p>
<h5 id="infModelParam">Inference for the Model Parameters</h5>
<p>Most inference in regression is focused on the slope, <span class="math inline">\(\beta_1\)</span>. Recall that the interpretation of <span class="math inline">\(\beta_1\)</span> is the amount of increase (or decrease) in the expected value of <span class="math inline">\(Y\)</span> per unit change in <span class="math inline">\(X\)</span>. There are three main scenarios where inference about the slope is of interest.</p>
<ol style="list-style-type: decimal">
<li><p>Determine if there is evidence of a meaningful linear relationship in the data. If <span class="math inline">\(\beta_1 = 0\)</span>, then there is no relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(E\{Y\}\)</span>. Hence we might be interested in testing the hypotheses <span class="math display">\[
  H_0: \beta_1 = 0
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq 0 
\]</span></p></li>
<li><p>Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> is some hypothesized number.</p></li>
<li><p>To provide a confidence interval for the true value of <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p><br /></p>
<div style="padding-left:15px;">
<h6 id="tTests">t Tests</h6>
<p>Before we discuss how to test the hypotheses listed above or construct a confidence interval, we must understand the distribution of the estimate <span class="math inline">\(b_1\)</span> of the parameter <span class="math inline">\(\beta_1\)</span>. Since <span class="math inline">\(b_1\)</span> is an estimate, it will vary from sample to sample, even though the truth, <span class="math inline">\(\beta_1\)</span>, remains fixed. It turns out that the sampling distribution of <span class="math inline">\(b_1\)</span> (where the <span class="math inline">\(X\)</span> values remain fixed from study to study) is normal with mean and variance: <span class="math display">\[
  \mu_{b_1} = \beta_1
\]</span> <span class="math display">\[
  \sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2}
\]</span> Hence, an immediate choice of statistical test to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10} 
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} 
\]</span> where <span class="math inline">\(\beta_{10}\)</span> can be zero, or any other value, is a t test given by <span class="math display">\[
  t = \frac{b_1 - \beta_{10}}{\sigma_{b_1}}
\]</span> where <span class="math inline">\(\sigma^2_{b_1} = \frac{MSE}{\sum(X_i-\bar{X})^2}\)</span>. With quite a bit of work it has been shown that <span class="math inline">\(t\)</span> is distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. The nearly identical test statistic for testing <span class="math display">\[
  H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_0 \neq \beta_{00} 
\]</span> is given by <span class="math display">\[
  t = \frac{b_0 - \beta_{00}}{\sigma_{b_0}}
\]</span> where <span class="math inline">\(\sigma^2_{b_0} = MSE\left[\frac{1}{n}+\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\)</span>. This version of <span class="math inline">\(t\)</span> has also been shown to be distributed as a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<p>Creating a confidence interval for either <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_0\)</span> follows immediately from these results using the formulas <span class="math display">\[
  b_1 \pm t^*_{n-2}\cdot \sigma_{b_1}
\]</span> <span class="math display">\[
  b_0 \pm t^*_{n-2}\cdot \sigma{b_0}
\]</span> where <span class="math inline">\(t^*_{n-2}\)</span> is the critical value from a t distribution with <span class="math inline">\(n-2\)</span> degrees of freedom corresponding to the chosen confidence level.</p>
<p><br /></p>
<h6 id="Ftests">F tests</h6>
<p>Another way to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}  \quad\quad \text{or} \quad\quad H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} \quad\quad \ \ \quad \quad H_a: \beta_0 \neq \beta_{00}
\]</span> is with an <span class="math inline">\(F\)</span> Test. One downside of the F test is that we cannot construct confidence intervals. Another is that we can only perform two-sided tests, we cannot use one-sided alternatives with an F test. The upside is that an <span class="math inline">\(F\)</span> test is very general and can be used in many places that a t test cannot.</p>
<p>In its most general form, the <span class="math inline">\(F\)</span> test partitions the sums of squared errors into different pieces and compares the pieces to see what is accounting for the most variation in the data. To test the hypothesis that <span class="math inline">\(H_0:\beta_1=0\)</span> against the alternative that <span class="math inline">\(H_a: \beta_1\neq 0\)</span>, we are essentially comparing two models against each other. If <span class="math inline">\(\beta_1=0\)</span>, then the corresponding model would be <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. If <span class="math inline">\(\beta_1\neq0\)</span>, then the model remains <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. We call the model corresponding to the null hypothesis the reduced model because it will always have fewer parameters than the model corresponding to the alternative hypothesis (which we call the full model). This is the first requirement of the <span class="math inline">\(F\)</span> Test, that the null model (reduced model) have fewer “free” parameters than the alternative model (full model). To demonstrate what we mean by “free” parameters, consider the following example.</p>
<p>Say we wanted to test the hypothesis that <span class="math inline">\(H_0:\beta_1 = 2.5\)</span> against the alternative that <span class="math inline">\(\beta_1\neq2.5\)</span>. Then the null, or reduced model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+2.5X_i\)</span>. The alternative, or full model, would be <span class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. Thus, the null (reduced) model contains only one “free” parameter because <span class="math inline">\(\beta_1\)</span> has been fixed to be 2.5 and is no longer free to be estimated from the data. The alternative (full) model contains two “free” parameters, both are to be estimated from the data. The null (reduced) model must contain fewer free parameters than the alternative (full) model.</p>
<p>Once the null and alternative models have been specified, the General Linear Test is performed by appropriately partitioning the squared errors into pieces corresponding to each model. In the first example where we were testing <span class="math inline">\(H_0: \beta_1=0\)</span> against <span class="math inline">\(H_a:\beta_1\neq0\)</span> we have the partition <span class="math display">\[
  \underbrace{Y_i-\bar{Y}}_{Total} = \underbrace{\hat{Y}_i - \bar{Y}}_{Regression} + \underbrace{Y_i-\hat{Y}_i}_{Error}
\]</span> The reason we use <span class="math inline">\(\bar{Y}\)</span> for the null model is that <span class="math inline">\(\bar{Y}\)</span> is the unbiased estimator of <span class="math inline">\(\beta_0\)</span> for the null model, <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. Thus we would compute the following sums of squares: <span class="math display">\[
  SSTO = \sum(Y_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSR = \sum(\hat{Y}_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSE = \sum(Y_i-\hat{Y}_i)^2
\]</span> and note that <span class="math inline">\(SSTO = SSR + SSE\)</span>. Important to note is that <span class="math inline">\(SSTO\)</span> uses the difference between the observations <span class="math inline">\(Y_i\)</span> and the null (reduced) model. The <span class="math inline">\(SSR\)</span> uses the diffences between the alternative (full) and null (reduced) model. The <span class="math inline">\(SSE\)</span> uses the differences between the observations <span class="math inline">\(Y_i\)</span> and the alternative (full) model. From these we could set up a General <span class="math inline">\(F\)</span> table of the form</p>
<table style="width:60%;">
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="6%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Sum Sq</th>
<th>Df</th>
<th>Mean Sq</th>
<th>F Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Error</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(df_R-df_F\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\cdot\frac{df_F}{SSE}\)</span></td>
</tr>
<tr class="even">
<td>Residual Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(df_F\)</span></td>
<td><span class="math inline">\(\frac{SSE}{df_F}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total Error</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(df_R\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
<h5 id="estMod">Estimating the Model Parameters</h5>
There are two approaches to estimating the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in model (). The oldest and most tradiational approach is using the idea of least squares. A more general approach uses the idea of maximum likelihood. Fortunately, for simple linear regression, the estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> obtained from either method are identical. The estimates for the true parameter values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are typically denoted by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, respectively, and are given by
<span class="math display">\[\begin{equation}
  b_0 = \frac{1}{n}\left(\sum Y_i - b_1\sum X_i\right) = \bar{Y} - b_1\bar{X}
  \label{bO}
\end{equation}\begin{equation}
  b_1 = \frac{\sum(X_i - \bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}
  \label{bI}
\end{equation}\]</span>
It is important to note that these estimates are entirely determined from the observed data <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. When the regression equation is written using the estimates instead of the parameters, we use the notation <span class="math inline">\(\hat{Y}\)</span>, which is the estimator of <span class="math inline">\(E\{Y\}\)</span>. Thus, we write
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1 X_i
\end{equation}\]</span>
which is directly comparable to the true, but unknown values
<span class="math display">\[\begin{equation}
  E\{Y_i\} = \beta_0 + \beta_1 X_i. 
  \label{exp}
\end{equation}\]</span>
<h5 id="leastSquares">Least Squares</h5>
<p>To estimate the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using least squares, we start by defining the function <span class="math inline">\(Q\)</span> as the sum of the squared errors, <span class="math inline">\(\epsilon_i\)</span>. <span class="math display">\[
  Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1 X_i))^2
\]</span> It is important to note that the function <span class="math inline">\(Q\)</span> is viewed as a function of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and that the values of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are considered fixed given a particular data set has been observed. Using calculus, we can take the partial derivatives of <span class="math inline">\(Q\)</span> with respect to both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. <span class="math display">\[
  \frac{\partial Q}{\partial \beta_0} = -2\sum (Y_i - \beta_0 - \beta_1X_i)
\]</span> <span class="math display">\[
  \frac{\partial Q}{\partial \beta_1} = -2\sum X_i(Y_i-\beta_0-\beta_1X_i)
\]</span> Setting these partial derivatives to zero, and solving the resulting system of equations provides the values of the parameters which minimize <span class="math inline">\(Q\)</span> for a given set of data. After all the calculations are completed we find the values of the parameter estimators <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> (of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively) are as stated previously.</p>
<h5 id="mle">Maximum Likelihood</h5>
<p>The idea of maximum likelihood estimation is opposite that of least squares. Instead of choosing those values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which minime the least squares <span class="math inline">\(Q\)</span> function, we choose the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> which maximize the likelihood function. The likelihood function is created by first determining the joint distribution of the <span class="math inline">\(Y_i\)</span> for all observations <span class="math inline">\(i=1,\ldots,n\)</span>. We can do this rather simply by using the assumption that the errors, <span class="math inline">\(\epsilon_i\)</span> are independently normally distributed. When events are independent, their joint probability is simply the product of their individual probabilities. Thus, if <span class="math inline">\(f(Y_i)\)</span> denotes the probability density function for <span class="math inline">\(Y_i\)</span>, then the joint probability density for all <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(f(Y_1,\ldots,Y_n)\)</span> is given by <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) 
\]</span> Since each <span class="math inline">\(Y_i\)</span> is assumed to be normally distributed with mean <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (see model ()) we have that <span class="math display">\[
  f(Y_i) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-\frac{1}{2}\left(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\right)^2\right]}
\]</span> which provides the joint probability as <span class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> The likelihood function <span class="math inline">\(L\)</span> is then given by consider the <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> fixed and the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span> as the variables in the function. <span class="math display">\[
  L(\beta_0,\beta_1,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> Instead of taking partial derivatives of <span class="math inline">\(L\)</span> directly (with respect to all parameters) we take the partial derivatives of the <span class="math inline">\(\log\)</span> of <span class="math inline">\(L\)</span>, which is easier to work with. In a similar, but more difficult calculation, to that of minimizing <span class="math inline">\(Q\)</span>, we obtain the values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span> which maximize the log of <span class="math inline">\(L\)</span>, and which therefore maximize <span class="math inline">\(L\)</span>. (This is not an obvious result, but can be verified after some intense calculations.) The additional result that maximimum likelihood estimation provides that the least squares estimates did not give us is the estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of <span class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
  \hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y}_i)^2}{n}
\]</span></p>
<h5 id="varEst">Estimating the Variance</h5>
As shown previously, we can obtain estimates for the model parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with either least squares estimation or maximum likelihood estimation. It turns out that these estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are nice in the sense that on average they provide the correct estimate of the true parameter, i.e., they are unbiased estimators. On the other hand, the maximum likelihood estimate <span class="math inline">\(\hat{\sigma}^2\)</span> of the model variance <span class="math inline">\(\sigma^2\)</span> is a biased estimator. It is consistently wrong in its estimates of <span class="math inline">\(\sigma^2\)</span>. Without going into all the details, <span class="math inline">\(\hat{\sigma}^2\)</span> is a biased estimator of <span class="math inline">\(\sigma^2\)</span> because its denominator needs to represent the degrees of freedom associated with the numerator. Since <span class="math inline">\(\hat{Y}_i\)</span> in the numerator of <span class="math inline">\(\hat{\sigma}^2\)</span> is defined by
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1X_i
  \label{hatY}
\end{equation}\]</span>
it follows that two means, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span>, must be estimated from the data to obtain <span class="math inline">\(\hat{Y}_i\)</span>, see formulas () and () for details. Anytime a mean is estimated from the data we lose a degree of freedom. Hence, the denominator for <span class="math inline">\(\hat{\sigma}^2\)</span> should be <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n\)</span>. Some incredibly long calculations will show that the estimator
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2}
\end{equation}\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Here <span class="math inline">\(MSE\)</span> stands for mean squared error, which is the most obvious name for a formula that squares the errors <span class="math inline">\(Y_i-\hat{Y}_i\)</span> then adds them up and divides by their degrees of freedom. Similarly, we call the numerator <span class="math inline">\(\sum(Y_i-\hat{Y}_i)^2\)</span> the sum of the squared errors, denoted by <span class="math inline">\(SSE\)</span>. It is also important to note that the errors are often denoted by <span class="math inline">\(e_i = Y_i-\hat{Y}_i\)</span>. Putting this all together we get the following equivalent statements for <span class="math inline">\(MSE\)</span>.
<span class="math display">\[\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2} = \frac{\sum e_i^2}{n-2}
\end{equation}\]</span>
<p>As a final note, even though the <span class="math inline">\(E\{MSE\} = \sigma^2\)</span>, <span class="math inline">\(MSE\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, it unfortunately isn’t true that <span class="math inline">\(\sqrt{MSE}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma\)</span>. This presents a few problems later on.</p>
<hr />
</div>
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/BodyWeightSLR.html">bodyweight</a>, <a href="./Analyses/carsSLR.html">cars</a></p>
</div>
<hr />
<div id="multiple-linear-regression" class="section level3 tabset tabset-fade tabset-pills">
<h3>Multiple Linear Regression</h3>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYMultX.png" width=108px;></p>
</div>
<p>Multiple regression allows for more than one explanatory variable to be included in the modeling of the expected value of the quantitative response variable <span class="math inline">\(Y_i\)</span>.</p>
<div id="overview-1" class="section level4">
<h4>Overview</h4>
<div style="padding-left:125px;">
<p>A typical multiple regression model is given by the equation <span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_p X_{pi} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_i\sim N(0,\sigma^2)\)</span>.</p>
<p>The coefficient <span class="math inline">\(\beta_j\)</span> is interpreted as the change in the expected value of <span class="math inline">\(Y\)</span> for a unit increase in <span class="math inline">\(X_{j}\)</span>, holding all other variables constant, for <span class="math inline">\(j=1,\ldots,p\)</span>.</p>
<p>See the <strong>Explanation</strong> tab for details about possible hypotheses here.</p>
<hr />
</div>
</div>
<div id="r-instructions-1" class="section level4">
<h4>R Instructions</h4>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p>Everything is the same as in simple linear regression except that more variables are allowed in the call to <code>lm()</code>.</p>
<p><code>mylm &lt;- lm(y ~ x1 + x2 + ... + xp + ..., data=YourDataSet)</code></p>
<ul>
<li><code>mylm</code> is some name you come up with to store the results of the <code>lm()</code> test. Note that <code>lm()</code> stands for “linear model.”</li>
<li><code>y</code> must be a “numeric” vector of the quantitative response variable.</li>
<li><code>x1</code>, <code>x2</code>, <code>...</code>, <code>xp</code> are the explanatory variables. These can either be quantitative or qualitative. Note that R treats “numeric” variables as quantitative and “character” or “factor” variables as qualitative. Further, when R thinks a variable is qualitative, then it creates a set of dummy variables that are each coded as 0,1 variables. It creates one fewer dummy variables than levels of the original qualitative variable.</li>
<li><code>YourDataSet</code> is the name of your data set.</li>
<li><code>...</code> interactions are also allowed.</li>
</ul>
<hr />
</div>
</div>
<div id="explanation-1" class="section level4">
<h4>Explanation</h4>
<div style="padding-left:125px;">
<p>The extension of linear regression to multiple regression is fairly direct yet very powerful. Multiple regression expands the simple regression model to include more explanatory variables. These extra variables are sometimes called <em>covariates</em>. Like simple regression, multiple regression still only allows for a single quantitative response variable.</p>
<h5 id="the-model">The Model</h5>
<p>The multiple linear regression model is given by <span class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_p X_{pi}}_{E\{Y\}} + \epsilon_i
\]</span> where <span class="math inline">\(\epsilon_i\sim N(0,\sigma^2)\)</span>. Thus, it is a direct extension of the simple linear regression model to the scenario where more than one explanatory variable can be included in the model.</p>
<div style="padding-left:15px; color:#a8a8a8;">
<p><strong>Note</strong>: Interactions, transformations of other variables, and qualitative variables can all be included in the model. For example, if a model included three explanatory variables, <span class="math inline">\(X_1,X_2,X_3\)</span>, then <span class="math inline">\(X_{3i}\)</span> could be defined to be the interaction between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span>, i.e., <span class="math inline">\(X_{3i} = X_{1i}\cdot X_{2i}\)</span>. If <span class="math inline">\(X_3\)</span> was instead to represent a qualitative variable with two levels, then we could use a 0 to represent one level of the variable and a 1 to represent the other level. If we had good reason to do so, we could even let <span class="math inline">\(X_{3i} = X_{1i}^2\)</span> or <span class="math inline">\(\log(X_{1i})\)</span> or some other transformation of another <span class="math inline">\(X\)</span> variable.</p>
</div>
<p>Say we are interested in the price of a vehicle, particularly the price of a Cadillac. Simple linear regression would just use the mileage of the vehicle to predict the price. This will probably not be very successful as different makes of Cadillacs vary widely in their prices. However, if we include other explanatory variables in our model, like model of the vehicle, we should be able to do very well at predicting the price of a particular vehicle. (Certainly other variables like the number of doors, engine size, automatic or manual transmission and so on could also be valuable explanatory variables.)</p>
<h5 id="interpretation">Interpretation</h5>
<p>The only change to interpretation from the simple linear regression model is that each coefficient, <span class="math inline">\(\beta_j\)</span> <span class="math inline">\(j=1,\ldots,p\)</span>, represents the change in the <span class="math inline">\(E\{Y\}\)</span> for a unit change in <span class="math inline">\(X_j\)</span>, <em>holding all other variables constant.</em></p>
<p><br /></p>
<h5 id="assumptions-1">Assumptions</h5>
<p>The assumptions of multiple linear regression are nearly identical to simple linear regression, with the addition of one new assumption.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered fixed and measured without error.</li>
<li>The error terms are independent.</li>
<li>All important variables are included in the model.</li>
</ol>
<p><br /></p>
<h5 id="check">Checking the Assumptions</h5>
<p>The process of checking assumptions is the same for multiple linear regression as it is for <a href="SimpleLinearRegression.html#check">simple linear regression</a>, with the addition of one more tool.</p>
<p>Added variable plots can be used to determine if a new variable should be included in the model.</p>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="144" />
</td>
<td width="75%">
<p>Let <span class="math inline">\(X_{new}\)</span> be a new explanatory variable that could be added to the current multiple regression model. Plotting the residuals from the current linear regression against <span class="math inline">\(X_{new}\)</span> allows us to determine if <span class="math inline">\(X_{new}\)</span> has any information to add to the current model. If there is a trend in the plot, then <span class="math inline">\(X_{new}\)</span> should be added to the model. If there is no trend in the plot, then the <span class="math inline">\(X_{new}\)</span> should be left out.</p>
<p>| <a href="./Analyses/AddedVariablePlot.html">Explanation</a> |</p>
</td>
</tr>
</table>
<p><br /></p>
<h5 id="infModelParam">Inference for the Model Parameters</h5>
<p>Inference in the multiple regression model can be for any of the model coefficients, <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_p\)</span> or for several coefficients simultaneously.</p>
<p><br /></p>
<h6 id="t-tests">t Tests</h6>
<p>The most typical tests for multiple regression are t Tests for a single coefficient. The hypotheses for these t Tests are written as <span class="math display">\[
  H_0: \beta_j = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0
\]</span> Note that these hypotheses assume that all other variables (and coefficients) are already in the model. The significance of the single variable is thus assessed after accounting for the effect of all other variables. If a t Test of a single coefficient is significant, then that variable should remain in the model. If the t Test for a single coefficient is not significant, then the other variables in the model provide the same information that the variable being tested provides. Removing it from the model may be appropriate. However, whenever a single variable is removed from the model the other variables can change in their significance.</p>
<p><br /></p>
<h6 id="f-tests">F Tests</h6>
<p>Another approach to testing hypotheses about coefficients is to use an F Test. The F Test allows a single test for any group of hypotheses simultaneously.</p>
<p>The most commonly used F Test is the one given by the hypotheses <span class="math display">\[
  H_0: \beta_0 = \beta_1 = \cdots = \beta_p = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0 \ \text{for at least one}\ j \in \{0,1,\ldots,p\}
\]</span> However, any subset of coefficients could be tested in a similar way using a customized F Test. The details of how to do this are somewhat involved and are beyond the scope of this class.</p>
<p><br /></p>
<h5 id="rsquared">Assessing the Model Fit</h5>
<p>There are many measures of the quality of a regression model. One of the most popular measurements is the <span class="math inline">\(R^2\)</span> value (“R-squared”). The <span class="math inline">\(R^2\)</span> value is a measure of the proportion of variation of the <span class="math inline">\(Y\)</span>-variable that is explained by the model. Specifically, <span class="math display">\[
  R^2 = \frac{\text{SSR}}{\text{SSTO}} = 1-\frac{\text{SSE}}{\text{SSTO}}
\]</span> The range of <span class="math inline">\(R^2\)</span> is between 0 and 1. Values close to 1 imply a very good model. Values close to 0 imply a very poor model.</p>
<p>One difficulty of <span class="math inline">\(R^2\)</span> in multiple regression is that it will always get larger when more variables are included in the regression model. Thus, in multiple linear regression, it is best to make an adjustment to the <span class="math inline">\(R^2\)</span> value to protect against this difficulty. The value of the adjusted <span class="math inline">\(R^2\)</span> is given by <span class="math display">\[
  R^2_{adj} = 1 - \frac{(n-1)}{(n-p)}\frac{\text{SSE}}{\text{SSTO}}
\]</span> The interpretation of <span class="math inline">\(R^2_{adj}\)</span> is essentially the same as the interpretation of <span class="math inline">\(R^2\)</span>, with the understanding that a correction has been made for the number of parameters included in the model, <span class="math inline">\((n-p)\)</span>.</p>
<p><br /> <br /></p>
<hr />
</div>
</div>
</div>
</div>
<div id="section-1" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a href="./Analyses/cadillacsMLR.html">cadillacs</a></p>
</div>
<hr />
<footer>
</footer>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
